{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8721ca84-f7cc-4173-9e53-5d9ee87031f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: spam.csv\n",
      "\n",
      "Loaded dataset successfully!\n",
      "Dataset shape: (5572, 5)\n",
      "Columns: ['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "\n",
      "First 5 rows:\n",
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "Column data types:\n",
      "v1            object\n",
      "v2            object\n",
      "Unnamed: 2    object\n",
      "Unnamed: 3    object\n",
      "Unnamed: 4    object\n",
      "dtype: object\n",
      "\n",
      "Using standard columns: v1 (label) and v2 (message)\n",
      "\n",
      "Missing values in each column:\n",
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "Dataset shape after dropping nulls: (5572, 2)\n",
      "\n",
      "Unique values in label column:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Converted labels to binary format:\n",
      "Mapping used: {'ham': 0, 'spam': 1}\n",
      "\n",
      "Label distribution after conversion:\n",
      "label\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Downloading NLTK stopwords...\n",
      "Download complete.\n",
      "\n",
      "Cleaning message text...\n",
      "\n",
      "Cleaned message examples:\n",
      "Original: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Cleaned: go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "--------------------------------------------------\n",
      "Original: Ok lar... Joking wif u oni...\n",
      "Cleaned: ok lar joking wif u oni\n",
      "--------------------------------------------------\n",
      "Original: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "Cleaned: free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry questionstd txt ratetcs apply overs\n",
      "--------------------------------------------------\n",
      "Original: U dun say so early hor... U c already then say...\n",
      "Cleaned: u dun say so early hor u c already then say\n",
      "--------------------------------------------------\n",
      "Original: Nah I don't think he goes to usf, he lives around here though\n",
      "Cleaned: nah i dont think he goes to usf he lives around here though\n",
      "--------------------------------------------------\n",
      "\n",
      "Features shape: (5572,)\n",
      "Target shape: (5572,)\n",
      "Cleaned dataset saved to 'cleaned_spam_dataset.csv'\n",
      "\n",
      "Class distribution:\n",
      "Ham (0): 4825 messages (86.59%)\n",
      "Spam (1): 747 messages (13.41%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Path to your locally saved file\n",
    "file_path = 'spam.csv'  # Adjust if the file is not in the current directory\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File {file_path} not found. Please check the path.\")\n",
    "else:\n",
    "    print(f\"File found: {file_path}\")\n",
    "    \n",
    "    # Try different encodings if needed\n",
    "    try:\n",
    "        # First try with latin-1 encoding (common for this dataset)\n",
    "        df = pd.read_csv(file_path, encoding='latin-1')\n",
    "    except UnicodeDecodeError:\n",
    "        # If that fails, try UTF-8\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # If both fail, try another common encoding\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(\"\\nLoaded dataset successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check types of columns\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Identify and keep only necessary columns (usually 'v1' is the label and 'v2' is the message)\n",
    "    # Common column patterns in this dataset:\n",
    "    if 'v1' in df.columns and 'v2' in df.columns:\n",
    "        print(\"\\nUsing standard columns: v1 (label) and v2 (message)\")\n",
    "        df = df[['v1', 'v2']]\n",
    "        df.columns = ['label', 'message']\n",
    "    elif 'type' in df.columns.str.lower().tolist() and 'text' in df.columns.str.lower().tolist():\n",
    "        # Some versions use 'type' and 'text'\n",
    "        label_col = df.columns[df.columns.str.lower() == 'type'][0]\n",
    "        message_col = df.columns[df.columns.str.lower() == 'text'][0]\n",
    "        print(f\"\\nUsing columns: {label_col} (label) and {message_col} (message)\")\n",
    "        df = df[[label_col, message_col]]\n",
    "        df.columns = ['label', 'message']\n",
    "    else:\n",
    "        # Try to identify label and message columns\n",
    "        label_col = None\n",
    "        message_col = None\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Sample some values from the column\n",
    "                sample_values = df[col].dropna().astype(str).unique()\n",
    "                \n",
    "                # Check if this looks like a label column (contains 'spam' or 'ham')\n",
    "                if any(v.lower() in ['spam', 'ham'] for v in sample_values):\n",
    "                    label_col = col\n",
    "                # Check if this looks like a message column (longer text)\n",
    "                elif df[col].astype(str).str.len().mean() > 20:\n",
    "                    message_col = col\n",
    "        \n",
    "        if label_col and message_col:\n",
    "            print(f\"\\nAutomatically identified columns: {label_col} (label) and {message_col} (message)\")\n",
    "            df = df[[label_col, message_col]]\n",
    "            df.columns = ['label', 'message']\n",
    "        else:\n",
    "            print(\"\\nCould not automatically identify label and message columns.\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            print(\"Please specify the columns manually in the code.\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Drop rows with missing values if any\n",
    "    df = df.dropna()\n",
    "    print(f\"Dataset shape after dropping nulls: {df.shape}\")\n",
    "    \n",
    "    # Check unique values in label column to confirm format\n",
    "    print(\"\\nUnique values in label column:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Convert labels to binary: ham = 0, spam = 1\n",
    "    # Check if conversion is needed\n",
    "    if df['label'].dtype == 'object':\n",
    "        # Check if values are already in correct format\n",
    "        if set(df['label'].unique()) == {0, 1}:\n",
    "            print(\"\\nLabels are already in binary format (0, 1)\")\n",
    "        else:\n",
    "            # Map string labels to binary\n",
    "            label_mapping = {}\n",
    "            for val in df['label'].unique():\n",
    "                if val.lower() == 'spam':\n",
    "                    label_mapping[val] = 1\n",
    "                else:  # Assuming everything else is 'ham'\n",
    "                    label_mapping[val] = 0\n",
    "            \n",
    "            df['label'] = df['label'].map(label_mapping)\n",
    "            print(\"\\nConverted labels to binary format:\")\n",
    "            print(f\"Mapping used: {label_mapping}\")\n",
    "    \n",
    "    print(\"\\nLabel distribution after conversion:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Download stopwords if needed\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"\\nDownloading NLTK stopwords...\")\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    # Text cleaning function\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Clean text by converting to lowercase, removing punctuation,\n",
    "        numbers, and extra whitespaces\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Apply cleaning to the message column\n",
    "    print(\"\\nCleaning message text...\")\n",
    "    df['clean_message'] = df['message'].apply(clean_text)\n",
    "    \n",
    "    # Display some cleaned messages\n",
    "    print(\"\\nCleaned message examples:\")\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"Original: {df.iloc[i]['message']}\")\n",
    "        print(f\"Cleaned: {df.iloc[i]['clean_message']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Split into features and target\n",
    "    X = df['clean_message']\n",
    "    y = df['label']\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    cleaned_path = 'cleaned_spam_dataset.csv'\n",
    "    df.to_csv(cleaned_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to '{cleaned_path}'\")\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(f\"Ham (0): {(y == 0).sum()} messages ({(y == 0).sum() / len(y):.2%})\")\n",
    "    print(f\"Spam (1): {(y == 1).sum()} messages ({(y == 1).sum() / len(y):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185dca1-5854-4966-8763-921d9a22815e",
   "metadata": {},
   "source": [
    "# SMS Spam Detection: Data Preparation Methodology\n",
    "\n",
    "## 1. File Loading & Validation\n",
    "- **Multiple encoding support**: Text data often contains special characters requiring specific encoding protocols. Testing multiple encodings (latin-1, utf-8, ISO-8859-1) ensures reliable loading regardless of source encoding.\n",
    "- **File existence check**: Validates data availability before processing, preventing mid-process failures.\n",
    "\n",
    "## 2. Dataset Structure Analysis\n",
    "- **Column identification**: SMS datasets may have inconsistent naming conventions (v1/v2, type/text). Intelligent column identification ensures processing works on any variant of the dataset.\n",
    "- **Data type verification**: Confirms appropriate data types for each column, preventing type-related errors during processing.\n",
    "\n",
    "## 3. Data Cleaning\n",
    "- **Missing value handling**: Removes incomplete records to ensure model training on complete data points only, improving reliability.\n",
    "- **Label standardization**: Converting text labels ('spam'/'ham') to binary format (1/0) is required for mathematical operations in machine learning algorithms.\n",
    "  \n",
    "## 4. Text Preprocessing\n",
    "- **Case normalization**: Converting all text to lowercase eliminates duplicate features from case variations (e.g., \"Free\" vs \"free\").\n",
    "- **Punctuation removal**: Punctuation rarely contributes to spam classification while increasing feature dimensionality.\n",
    "- **Number removal**: Numbers in SMS messages are typically contextual and don't strongly indicate spam status independently.\n",
    "- **Whitespace normalization**: Standardizes spacing to improve token extraction consistency.\n",
    "\n",
    "## 5. Data Export\n",
    "- **Creating cleaned dataset file**: Preserves the processed data for direct use in subsequent modeling steps without repeating preprocessing.\n",
    "- **Class distribution analysis**: Documents the dataset balance, essential for selecting appropriate evaluation metrics and sampling techniques.\n",
    "\n",
    "## Technical Justification\n",
    "Each preprocessing step reduces noise while preserving signal, lowering dimensionality while maintaining classification-relevant information. This improves both model training efficiency and generalization performance on unseen data.\n",
    "\n",
    "The approach follows established NLP preprocessing best practices while being specifically tailored to the spam detection domain, where linguistic patterns rather than individual numbers or special characters typically indicate spam content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd6de4-b280-4212-98fe-78cbea11cd73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
